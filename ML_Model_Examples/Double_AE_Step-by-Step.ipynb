{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the good data\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import json\n",
    "\n",
    "df= pd.read_csv('Tot_ZeroBias_UL2017_DataFrame_chargeInner_PXLayer_1.csv')#'GOLDEN_Tot_ZeroBias_UL2017_DataFrame_chargeInner_PXLayer_1.csv')\n",
    "\n",
    "df['histo']=df['histo'].apply(literal_eval)\n",
    "\n",
    "df.set_index(['fromrun','fromlumi'], inplace=True, drop=False)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "print(df.tail())\n",
    "print(df.fromrun.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Golden JSON labels to the DF\n",
    "import json\n",
    "\n",
    "def checkLS(run,ls):\n",
    "    isok=False\n",
    "    global jsondata\n",
    "    \n",
    "    if str(run) in jsondata.keys():\n",
    "        for i in jsondata[str(run)]:\n",
    "            if (ls>=i[0] and ls <=i[1]):\n",
    "                isok=True\n",
    "                return isok\n",
    "        return isok\n",
    "\n",
    "#load the golden json file\n",
    "jsondata={}\n",
    "with open('GoldenJSON17.json') as json_file:\n",
    "    jsondata = json.load(json_file)\n",
    "\n",
    "df['labels']=False #initialize to false\n",
    "\n",
    "for run in df['fromrun'].unique():\n",
    "    for ls in df['fromlumi'][run]:\n",
    "        df['labels'][run][ls]=checkLS(run,ls)\n",
    "\n",
    "#print(df[df['labels']==True]) #to check against the Golden JSON\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract test and train data and put in dataframes\n",
    "\n",
    "#df_train=df.loc[df['labels']==True]\n",
    "#df_test=df.loc[df['labels']==False]\n",
    "\n",
    "X_trainS = np.stack(df['histo'][df['labels']==True].values, axis=0) #convert list of array to a stack to feed the model\n",
    "X_testS = np.stack(df['histo'][df['labels']==False].values, axis=0)\n",
    "\n",
    "#print(X_trainS)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train=normalize(X_trainS, norm='l1', axis=1) #normalise the sample, i.e the rows\n",
    "X_test=normalize(X_testS, norm='l1', axis=1)\n",
    "                      \n",
    "df['origin']=''\n",
    "df['origin'].loc[df['labels']==True]=X_train.tolist()\n",
    "df['origin'].loc[df['labels']==False]=X_test.tolist()\n",
    "print(df.shape)\n",
    "print(df.origin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "import math\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def mseTop10(y_true, y_pred):\n",
    "    top_values, _ = tf.nn.top_k(K.square(y_pred - y_true), k=10, sorted=True)\n",
    "    mean=K.mean(top_values, axis=-1)\n",
    "    return mean\n",
    "\n",
    "input_size=len(X_train[0])\n",
    "\n",
    "input_layer = Input(shape=(input_size, ))\n",
    "\n",
    "encoded = Dense(10, activation='tanh')(input_layer)\n",
    "encoded1 = Dense(3, activation='tanh')(encoded)\n",
    "encoded2 = Dense(10, activation='tanh')(encoded1)\n",
    "decoder = Dense(input_size, activation='tanh')(encoded2)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss=mseTop10)\n",
    "\n",
    "for i, layer in enumerate(autoencoder.layers):\n",
    "             layer.name = 'layer_' + str(i)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(X_train,X_train, epochs=100, batch_size=500, shuffle=False, verbose=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(history)\n",
    "\n",
    "line_styles = [(0, ()), (0, (1, 3)), (0, (3, 2))]\n",
    "\n",
    "color_palette = {\"Indigo\": {\n",
    "                    50: \"#E8EAF6\",\n",
    "                    100: \"#C5CAE9\",\n",
    "                    200: \"#9FA8DA\",\n",
    "                    300: \"#7986CB\",\n",
    "                    400: \"#5C6BC0\",\n",
    "                    500: \"#3F51B5\",\n",
    "                    600: \"#3949AB\",\n",
    "                    700: \"#303F9F\",\n",
    "                    800: \"#283593\",\n",
    "                    900: \"#1A237E\"},\n",
    "                 \"Teal\": {      \n",
    "                    50: \"#E0F2F1\",\n",
    "                    100: \"#B2DFDB\",\n",
    "                    200: \"#80CBC4\",\n",
    "                    300: \"#4DB6AC\",\n",
    "                    400: \"#26A69A\",\n",
    "                    500: \"#009688\",\n",
    "                    600: \"#00897B\",\n",
    "                    700: \"#00796B\",\n",
    "                    800: \"#00695C\",\n",
    "                    900: \"#004D40\"}\n",
    "                }\n",
    "\n",
    "def plot_loss(data, title):\n",
    "    \"\"\"Plots the training and validation loss\"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.plot(data.history['loss'], linestyle=line_styles[0], color=color_palette[\"Indigo\"][900], linewidth=3)\n",
    "    plt.plot(data.history['val_loss'], linestyle=line_styles[2], color=color_palette[\"Teal\"][300], linewidth=3)\n",
    "    plt.legend([\"Train\", \"Validation\"], loc=\"upper right\", frameon=False)\n",
    "    #plt.ylim(8258339,8258400)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlim(0,100)\n",
    "    plt.show();\n",
    "\n",
    "plot_loss(history, \"Original model loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTest=autoencoder.predict(X_test)\n",
    "predictionTrain=autoencoder.predict(X_train)\n",
    "\n",
    "mseTest=K.eval(mseTop10(X_test, predictionTest))\n",
    "mseTrain=K.eval(mseTop10(X_train, predictionTrain))\n",
    "\n",
    "df['prediction']=\"\"\n",
    "df['mse']=\"\"\n",
    "\n",
    "df['prediction'].loc[df['labels']==True]=predictionTrain.tolist()\n",
    "df['prediction'].loc[df['labels']==False]=predictionTest.tolist()\n",
    "\n",
    "df['mse'].loc[df['labels']==True]=mseTrain\n",
    "df['mse'].loc[df['labels']==False]=mseTest\n",
    "\n",
    "print(df.head())\n",
    "#print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def globalMSETrend(df,type=1):\n",
    "    \n",
    "    y=[]\n",
    "    \n",
    "    rmax=0\n",
    "    rmin=10\n",
    "    \n",
    "    print(rmax, rmin)\n",
    "    for run in df['fromrun'].unique():\n",
    "        if (type==1): val=(df['mse'][run]).mean()\n",
    "        if (type==2): val=(df['mse2'][run]).mean()\n",
    "        if val > rmax: rmax=val\n",
    "        if val < rmin: rmin =val\n",
    "        y.append(val)\n",
    "    \n",
    "    array=np.array(y)\n",
    "    gmean=array.mean()\n",
    "    size=float(array.size)\n",
    "    gstd=array.std()\n",
    "    print(size)\n",
    "    \n",
    "    fig=plt.figure()\n",
    "    fig.set_size_inches(16, 9)\n",
    "    \n",
    "    plt.hlines(gmean,df['fromrun'].unique().min(), df['fromrun'].unique().max(), color=\"blue\", label=\"Run average: \" + str(gmean))\n",
    "    plt.hlines(gmean+(1.0*gstd), df['fromrun'].unique().min(), df['fromrun'].unique().max(), color=\"red\", label='1 SD (' + str(gstd) + \")\")\n",
    "    plt.hlines(gmean+(3.0*gstd), df['fromrun'].unique().min(), df['fromrun'].unique().max(), color=\"red\", label='3 SD', linestyle=':')\n",
    "    \n",
    "    plt.ylim(rmin*0.9,rmax*1.1)\n",
    "    plt.scatter(df['fromrun'].unique(), y, marker='+', label='Data points')\n",
    "    plt.xlabel(\"Run\")\n",
    "    plt.ylabel(\"average MSE\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalMSETrend(df[df.labels==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckPredictions(df_test, run, ls):\n",
    "\n",
    "    Xmax=80000.0\n",
    "    Xmin=0.0\n",
    "    Xbins=100\n",
    "    hname=\"chargeInner_PXLayer_1\"\n",
    "    \n",
    "    ahisto=df_test['origin'][run][ls]\n",
    "    ahisto1=df_test['prediction'][run][ls]\n",
    "    mse=df_test['mse'][run][ls]\n",
    "    \n",
    "    x= np.linspace(Xmin,Xmax,Xbins)\n",
    "    #plt.xlim(Xmin,Xmax)\n",
    "    \n",
    "    plt.step(x, ahisto, where='mid', label=(\" Data \" + str(df_test.fromlumi[run][ls]) + \" Run \" + str(df_test.fromrun[run][ls]) ))\n",
    "    plt.step(x, ahisto1, where='mid', label=\" Reco MSE=\" + str(mse))\n",
    "    plt.xlabel(\"Charge electrons\")\n",
    "    plt.ylabel(\"A.U\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in df['fromrun'].unique():\n",
    "    count=0\n",
    "    for ls in df['fromlumi'][run]: \n",
    "        if df['train'][run][ls]== False: continue\n",
    "        if (count >2): continue\n",
    "        count=count+1\n",
    "        CheckPredictions(df[df.train==True], run,ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layerA = Input(shape=(input_size, ))\n",
    "\n",
    "encodedA = Dense(20, activation='sigmoid')(input_layerA)\n",
    "encoded1A = Dense(10, activation='tanh')(encodedA)\n",
    "encoded2A = Dense(20, activation='sigmoid')(encoded1A)\n",
    "decoderA = Dense(input_size, activation='sigmoid')(encoded2A)\n",
    "\n",
    "autoencoder_Opt = Model(inputs=input_layerA, outputs=decoderA)\n",
    "autoencoder_Opt.compile(optimizer='adam', loss=mseTop10)\n",
    "\n",
    "for i, layer in enumerate(autoencoder_Opt.layers):\n",
    "             layer.name = 'layer_' + str(i)\n",
    "\n",
    "autoencoder_Opt.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainA = np.stack(df['origin'][df['train']==True].values, axis=0) #convert list of array to a stack to feed the model\n",
    "#no need to normalize, we already have the normalized histogram in the df\n",
    "print(X_trainA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_Opt = autoencoder_Opt.fit(X_trainA,X_trainA, epochs=300, batch_size=100, shuffle=True, verbose=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate predictions and mse and add to original DF\n",
    "\n",
    "originS=np.stack(df['origin'].values, axis=0)\n",
    "predictionA=autoencoder_Opt.predict(originS)\n",
    "\n",
    "mseA=K.eval(mseTop10(originS,predictionA))\n",
    "\n",
    "df['predictions2']=\"\"\n",
    "df['mse2']=\"\"\n",
    "\n",
    "df['prediction2']=predictionA.tolist()\n",
    "df['mse2']=mseA\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckAllPredictions(df_test, run, ls):\n",
    "\n",
    "    Xmax=80000.0\n",
    "    Xmin=0.0\n",
    "    Xbins=100\n",
    "    hname=\"chargeInner_PXLayer_1\"\n",
    "    \n",
    "    ahisto=df_test['origin'][run][ls]\n",
    "    ahisto1=df_test['prediction'][run][ls]\n",
    "    ahisto2=df_test['prediction2'][run][ls]\n",
    "    mse=df_test['mse'][run][ls]\n",
    "    mse2=df_test['mse2'][run][ls]\n",
    "    \n",
    "    x= np.linspace(Xmin,Xmax,Xbins)\n",
    "    #plt.xlim(Xmin,Xmax)\n",
    "    \n",
    "    plt.step(x, ahisto, where='mid', label=(\" Data \" + str(df_test.fromlumi[run][ls]) + \" Run \" + str(df_test.fromrun[run][ls]) ))\n",
    "    plt.step(x, ahisto1, where='mid', label=\" Raw Reco, mse:\" + str(mse))\n",
    "    plt.step(x, ahisto2, where='mid', label=\" Reco, mse:\" + str(mse2))\n",
    "    plt.xlabel(\"Charge electrons\")\n",
    "    plt.ylabel(\"A.U\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare some old and new predictions\n",
    "\n",
    "for run in df['fromrun'].unique():\n",
    "    count=0\n",
    "    if run <298000: continue\n",
    "    for ls in df['fromlumi'][run]: \n",
    "        if df['train'][run][ls]== True: continue\n",
    "        if df['mse2'][run][ls]<0.00001: continue\n",
    "        if df['entries'][run][ls]<3000: continue\n",
    "        #if (count >2): continue\n",
    "        count=count+1\n",
    "        #print(df.entries)[run][ls]\n",
    "        CheckAllPredictions(df[df.train==False], run,ls)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
