{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = \"/nfs/public/mschneid/moredqmiodata/_SingleMuon_Run2018A-12Nov2019_UL2018-v2_DQMIO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/04\n"
     ]
    }
   ],
   "source": [
    "# in this notebook, we try to apply some operations (namely, computing the\n",
    "# covariance matrix for PCA) to a large amount of MEs at once. This requires\n",
    "# a lot more control over IO, to make sure we get efficient reads and also\n",
    "# don't run out of memory on the way.\n",
    "\n",
    "import os\n",
    "# it is cool that numpy uses multiple threads here, but for the small matrices it is not very efficient...\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" \n",
    "\n",
    "import ROOT\n",
    "import root_numpy\n",
    "from collections import namedtuple, defaultdict\n",
    "IndexEntry = namedtuple('IndexEntry', ['run', 'lumi', 'type', 'file', 'firstidx', 'lastidx'])\n",
    "MonitorElement = namedtuple('MonitorElement', ['run', 'lumi', 'name', 'type', 'data'])\n",
    "NTHREADS=128\n",
    "\n",
    "def extractdatafromROOT(x):\n",
    "    if isinstance(x, ROOT.string):\n",
    "        return unicode(x.data())\n",
    "    if isinstance(x, int):\n",
    "        return x\n",
    "    if isinstance(x, float):\n",
    "        return x\n",
    "    else:\n",
    "        return root_numpy.hist2array(x)\n",
    "    \n",
    "class DQMIOFile:\n",
    "    \"\"\"\n",
    "    Open the passed in file and read index data.\n",
    "    The file names go directly to ROOT, remote files like root://cms-xrd-global.cern.ch//store/... should work.\n",
    "    \"\"\"\n",
    "    def __init__(self, rootfile):\n",
    "        # for non-local files (/store/...), prefix them with \"root://cms-xrd-global.cern.ch/\"\n",
    "        self.rootfile = ROOT.TFile.Open(rootfile)\n",
    "        self.readindex()\n",
    "        \n",
    "    \"\"\"\n",
    "    Internal: read index tables.\n",
    "    \"\"\"\n",
    "    def readindex(self):\n",
    "        self.index = defaultdict(list)\n",
    "        idxtree = getattr(self.rootfile, \"Indices\")\n",
    "        # release GIL in long operations. Disable if it causes trouble.\n",
    "        #idxtree.GetEntry._threaded = True\n",
    "\n",
    "        for i in range(idxtree.GetEntries()):\n",
    "            idxtree.GetEntry(i)\n",
    "            run, lumi, metype = idxtree.Run, idxtree.Lumi, idxtree.Type\n",
    "            if lumi == 0:\n",
    "                # read only per-lumi MEs for now.\n",
    "                continue\n",
    "            # inclusive range -- for 0 entries, row is left out\n",
    "            firstidx, lastidx = idxtree.FirstIndex, idxtree.LastIndex\n",
    "            e = IndexEntry(run, lumi, metype, self.rootfile.GetName(), firstidx, lastidx)\n",
    "            self.index[(run, lumi)].append(e)\n",
    "    \n",
    "\"\"\"\n",
    "Read MEs matching the given wildcard patterns from a single lumi.\n",
    "For rootobjects = True, return actual root objects, by default the histogram data is extracted into numpy arrays.\n",
    "Returns a list of MonitorElement named tuples.\n",
    "\"\"\"\n",
    "def getMEsForLumi(entries, nameset):\n",
    "    treenames = { \n",
    "      0: \"Ints\",\n",
    "      1: \"Floats\",\n",
    "      2: \"Strings\",\n",
    "      3: \"TH1Fs\",\n",
    "      4: \"TH1Ss\",\n",
    "      5: \"TH1Ds\",\n",
    "      6: \"TH2Fs\",\n",
    "      7: \"TH2Ss\",\n",
    "      8: \"TH2Ds\",\n",
    "      9: \"TH3Fs\",\n",
    "      10: \"TProfiles\",\n",
    "      11: \"TProfile2Ds\",\n",
    "    }\n",
    "    rootfile = ROOT.TFile.Open(entries[0].file)\n",
    "    result = []\n",
    "    for e in entries:\n",
    "        metree = getattr(rootfile, treenames[e.type])\n",
    "        metree.GetEntry(0)\n",
    "        metree.SetBranchStatus(\"*\",0)\n",
    "        metree.SetBranchStatus(\"FullName\",1)\n",
    "        # release GIL in long operations. Disable if it causes trouble.\n",
    "        metree.GetEntry._threaded = True\n",
    "        for x in range(e.firstidx, e.lastidx+1):\n",
    "            metree.GetEntry(x)\n",
    "            mename = str(metree.FullName)\n",
    "            if not nameset or mename in nameset:\n",
    "                metree.GetEntry(x, 1)\n",
    "                value = metree.Value\n",
    "                value = extractdatafromROOT(value)\n",
    "                me = MonitorElement(e.run, e.lumi, mename, e.type, value)\n",
    "                result.append(me)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open all the files, and read their indices. We don't keep the reader objects;\n",
    "# the actual IO is a free function that allows us more control over parallelism.\n",
    "\n",
    "from glob import glob\n",
    "files = glob(DATAPATH + \"/*.root\")[:50]\n",
    "\n",
    "def entries(f):\n",
    "    iofile = DQMIOFile(f)\n",
    "    return list(iofile.index.values())\n",
    "\n",
    "es = sum(map(entries, files), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try reading a lumisection. \"None\" means read all MEs.\n",
    "mes = getMEsForLumi(es[0], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27668"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can slim down the list a bit to reduce the IO volume. E.g. we can't apply\n",
    "# PCA to big 2D histograms anyways.\n",
    "import numpy\n",
    "interesting = set([x.name for x in mes if numpy.prod(x.data.shape) < 1000 \n",
    "     and not x.name.startswith(\"Pixel\") \n",
    "     #and not x.name.startswith(\"RPC\")\n",
    "     and not x.name.startswith(\"JetMet\")\n",
    "     and not x.name.startswith(\"HLT\")\n",
    "     and not x.name.startswith(\"L1TEMU\")\n",
    "     #and not x.name.startswith(\"Egamma\")\n",
    "     and not x.name.startswith(\"AlCaReco\")])\n",
    "# the result is a set, which allows fast lookups. \n",
    "len(interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 s, sys: 173 ms, total: 3.18 s\n",
      "Wall time: 3.79 s\n"
     ]
    }
   ],
   "source": [
    "# try reading the reduced set, it should be fast.\n",
    "%time mes = getMEsForLumi(es[100], interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27668"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... and make sure we actually got something.\n",
    "len(mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'392 reading tasks, 24477 covariance tasks remaining'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now do the actual work. We will use a mulitprocessing process Pool to read many\n",
    "# files in parallel (the process isolation helps to keep ROOT from blowing up, and\n",
    "# also helps with the operations that may be CPU-bound on Python side), as well as \n",
    "# a thread pool for the actual covariance computation (here, the threadpool reduces\n",
    "# IO overhead, since the actual amount of work to do is rather small.)\n",
    "# This requires a bit of juggling with Tasks and Promises, since we can't just read\n",
    "# everything first (we might run out of memory.)\n",
    "# To make the covariance computation fast, it is crucial to sort the data by ME first.\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool \n",
    "\n",
    "def extractfile(es_names):\n",
    "    return getMEsForLumi(*es_names)\n",
    "\n",
    "covmat = dict()\n",
    "covqueue = defaultdict(list)\n",
    "\n",
    "def addtocovmat(mes):\n",
    "    # the bottleneck here are actually the stack calls, which do not release the GIL.\n",
    "    # the .dot() which does is pretty fast. So using multiple threads does not gain much...\n",
    "    assert(len(set(me.name for me in mes)) == 1)\n",
    "    if len(mes[0].data.shape) > 1:\n",
    "        # the reshape call is rather slow, avoid it if we can\n",
    "        size = numpy.prod(mes[0].data.shape)\n",
    "        mat = numpy.stack([me.data.reshape(size) for me in mes])\n",
    "    else:\n",
    "        mat = numpy.stack([me.data for me in mes])\n",
    "    cmat = mat.transpose().dot(mat)\n",
    "    if not mes[0].name in covmat:\n",
    "        covmat[mes[0].name] = cmat\n",
    "    else:\n",
    "        covmat[mes[0].name] += cmat\n",
    "            \n",
    "def addtocovqueue(mes):\n",
    "    for me in mes:\n",
    "        covqueue[me.name].append(me)\n",
    "        \n",
    "def processcovqueue(tp, minbatchsize = 0):\n",
    "    res = []\n",
    "    keys = covqueue.keys()\n",
    "    for mename in keys:\n",
    "        if len(covqueue[mename]) > minbatchsize:\n",
    "            res.append(tp.map_async(addtocovmat, [covqueue[mename]]))\n",
    "            covqueue[mename] = []\n",
    "    return res\n",
    "    \n",
    "\n",
    "p = Pool(64)\n",
    "tp = ThreadPool(4)\n",
    "\n",
    "res = []\n",
    "for e in es:\n",
    "    res.append(p.map_async(extractfile, [(e, interesting),]))\n",
    "\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "covres = []\n",
    "\n",
    "while len(res) > 0 or len(covres) > 0:\n",
    "    done = []\n",
    "    covdone = []\n",
    "    for r in res:\n",
    "        if r.ready():\n",
    "            map(addtocovqueue, r.get())\n",
    "            done.append(r)\n",
    "    for r in covres:\n",
    "        if r.ready():\n",
    "            covdone.append(r)\n",
    "    if len(covres) == 0 or len(res) == 0:\n",
    "        # batch up until we run out of work...\n",
    "        covres += processcovqueue(tp, 0)\n",
    "    res = [r for r in res if r not in done]\n",
    "    covres = [r for r in covres if r not in covdone]\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(\"%d reading tasks, %d covariance tasks remaining\" % (len(res), len(covres)))\n",
    "    time.sleep(1)\n",
    "    \n",
    "# wait for async stuff to complete\n",
    "p.close()\n",
    "tp.close()\n",
    "p.join()\n",
    "tp.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a random covariance matrix\n",
    "covmat[covmat.keys()[4567]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we have all...\n",
    "covmat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we can play around with a much smaller dataset...\n",
    "eigval, eigvec = numpy.linalg.eig(covmat[covmat.keys()[4567]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [15, 8]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(numpy.log(eigval), 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for mename in random.sample(covmat.keys(), 5):\n",
    "    eigval, eigvec = numpy.linalg.eig(covmat[mename])\n",
    "    plt.plot(numpy.log(eigval), 'o-', label=mename)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
